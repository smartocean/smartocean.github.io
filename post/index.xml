<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | 智慧海洋实验室</title>
    <link>https://smartocean.github.io/post/</link>
      <atom:link href="https://smartocean.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh-Hans</language><lastBuildDate>Tue, 25 Apr 2023 16:46:41 +0800</lastBuildDate>
    <image>
      <url>https://smartocean.github.io/media/icon_hu6833a01f336ec596d9b42e0f341d101a_74137_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://smartocean.github.io/post/</link>
    </image>
    
    <item>
      <title>基于Mnist Fashion数据集的卷积神经网络</title>
      <link>https://smartocean.github.io/post/mnist-fashion-cnn-network/</link>
      <pubDate>Tue, 25 Apr 2023 16:46:41 +0800</pubDate>
      <guid>https://smartocean.github.io/post/mnist-fashion-cnn-network/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文首发于&lt;a href=&#34;https://www.everains.com/note/ar339.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;牧雨博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.everains.com/note/ar339.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.everains.com/note/ar339.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;问题重述&#34;&gt;问题重述&lt;/h2&gt;
&lt;p&gt;构建卷积神经网络模型（CNN）对fashion-mnist数据集进行分类(10类)。&lt;/p&gt;
&lt;h2 id=&#34;数据简介&#34;&gt;数据简介&lt;/h2&gt;
&lt;p&gt;FashionMNIST 是一个替代 &lt;a href=&#34;https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/mnist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MNIST 手写数字集&lt;/a&gt;的图像数据集。 它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片,分别是：t-shirt（T恤），trouser（牛仔裤），pullover（套衫），dress（裙子），coat（外套）,sandal（凉鞋），shirt（衬衫），sneaker（运动鞋），bag（包），ankle boot（短靴）。&lt;/p&gt;
&lt;p&gt;FashionMNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。60000/10000 的训练测试数据划分，28×28 的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，&lt;strong&gt;且不需要改动任何的代码&lt;/strong&gt;（实测跑mnist的模型用在fashion上的准确率能到60%）。&lt;/p&gt;
&lt;p&gt;这个数据集的样子大致如下（每个类别占三行）：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://pic2.zhimg.com/v2-2eef619bec492f9c054b22c8f0bafcc9_r.jpg&#34; alt=&#34;preview&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;数据读取及预处理&#34;&gt;数据读取及预处理&lt;/h2&gt;
&lt;p&gt;使用Keras模块可以轻松导入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;from keras.datasets import fashion_mnist
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)/255
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)/255
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;y_train = to_categorical(y_train)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;y_test = to_categorical(y_test)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中，首先使用reshape函数将x_train,x_test张量形状转为(60000, 28, 28, 1),(10000, 28, 28, 1)这一四阶张量,这意味着训练样本数量为60000份测试样本数量为10000份，每张样本图像大小为28*28。由于每个样本的像素值是0至255之间的值，因此在训练网络之前进行了归一化的操作。&lt;/p&gt;
&lt;p&gt;而y_train,y_test则存储每个样本所对应的服装种类，用1~10表示。&lt;/p&gt;
&lt;h2 id=&#34;模型建立&#34;&gt;模型建立&lt;/h2&gt;
&lt;p&gt;模型方面，参考了VGG网络结构[^1]，大致特征为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用连续的小卷积核(3×3)做连续卷积，卷积的固定步长为1，并在图像的边缘填充1个像素，这样卷积后保持图像的分辨率不变。&lt;/li&gt;
&lt;li&gt;连续的卷积层会接着一个池化层，降低图像的分辨率。空间池化由两个最大池化层进行，并在2×2像素窗口上进行最大池化。&lt;/li&gt;
&lt;li&gt;卷积层后，接着的是3个全连接层，前两个分别为512、64个通道，第三是输出层输出10个分类。&lt;/li&gt;
&lt;li&gt;每个隐藏层的激活函数均使用ReLU。&lt;/li&gt;
&lt;li&gt;使用多次Dropout，以避免多次训练后的过拟合问题（具体表现为损失率后期增高）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下为网络详细结构&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Model: &amp;#34;sequential_1&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Layer (type)                 Output Shape              Param #   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;==============================================================
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_7 (Conv2D)            (None, 28, 28, 32)        9248      
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dropout_5 (Dropout)          (None, 28, 28, 32)        0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_8 (Conv2D)            (None, 28, 28, 64)        18496     
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dropout_6 (Dropout)          (None, 14, 14, 64)        0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_9 (Conv2D)            (None, 14, 14, 128)       73856     
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_10 (Conv2D)           (None, 14, 14, 128)       147584    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conv2d_11 (Conv2D)           (None, 14, 14, 256)       295168    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;max_pooling2d_3 (MaxPooling2 (None, 7, 7, 256)         0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dropout_7 (Dropout)          (None, 7, 7, 256)         0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;flatten_1 (Flatten)          (None, 12544)             0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dense_3 (Dense)              (None, 512)               6423040   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dropout_8 (Dropout)          (None, 512)               0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dense_4 (Dense)              (None, 64)                32832     
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dropout_9 (Dropout)          (None, 64)                0         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;______________________________________________________________
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dense_5 (Dense)              (None, 10)                650       
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;==============================================================
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Total params: 7,001,194
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Trainable params: 7,001,194
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Non-trainable params: 0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;分析结果&#34;&gt;分析结果&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.everains.com/images/2021/10/15/05410b49d6d1245c14e7d0ff4611091f.png&#34; alt=&#34;下载&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;训练精度&lt;/th&gt;
&lt;th&gt;泛化精度&lt;/th&gt;
&lt;th&gt;训练误差&lt;/th&gt;
&lt;th&gt;泛化误差&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.9684&lt;/td&gt;
&lt;td&gt;0.9304&lt;/td&gt;
&lt;td&gt;0.0909&lt;/td&gt;
&lt;td&gt;0.2591&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可以看出，各精度指标均在93%以上，模型较好。&lt;/p&gt;
&lt;p&gt;但通过图表可以观察到，在第10次学习后，泛化误差开始明显上升，说明出现过拟合问题，这一点或许可以通过增加Dropout、加大网络深度及降低学习率解决。&lt;/p&gt;
&lt;h2 id=&#34;优化方向&#34;&gt;优化方向&lt;/h2&gt;
&lt;p&gt;在进行卷积操作时，本模型使用的方法是采用边界填充（Padding）操作，在图像外围填充数值0，再进行卷积操作，经过一次卷积后输出的特征图矩阵与输入的图像矩阵有相同的大小，解决训练深度受限的问题.但此方法会使图像边缘信息模糊，导致在一定程度上降低准确度。而如果在每一步对padding操作进行权重化，根据相关研究，可能将准确度提升至多1.52%[^2]。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;[^1]: [1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;br&gt;
[^2]: 刘之瑜. 基于Padding权重化的卷积神经网络研究[J]. New Generation of Information Technology, 2021, 4(3):14-20.&lt;/p&gt;
&lt;p&gt;[^3]: &lt;a href=&#34;https://www.kaggle.com/gpreda/cnn-with-tensorflow-keras-for-fashion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kaggle.com/gpreda/cnn-with-tensorflow-keras-for-fashion&lt;/a&gt; Accessed at 2021/10/15&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
